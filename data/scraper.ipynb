{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def extract_hrefs_from_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all('article', id=True)\n",
    "    return [article.find('a')['href'] for article in articles if article.find('a')]\n",
    "\n",
    "def extract_blog_post_data(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1', class_='entry-title')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "    # Extract date in YYYY-MM-DD format\n",
    "    time_tag = soup.find('time', class_='entry-date published')\n",
    "    post_date = time_tag['datetime'].split('T')[0] if time_tag and 'datetime' in time_tag.attrs else \"\"\n",
    "\n",
    "    # Extract content\n",
    "    content_div = soup.find('div', class_='entry-content')\n",
    "    content_lines = []\n",
    "\n",
    "    if content_div:\n",
    "        for element in content_div.descendants:\n",
    "            if element.name == 'p':\n",
    "                text = element.get_text(separator=\" \", strip=True)\n",
    "                if text:\n",
    "                    content_lines.append(text)\n",
    "            elif element.name == 'li':\n",
    "                li_text = element.get_text(separator=\" \", strip=True)\n",
    "                if li_text:\n",
    "                    content_lines.append(f\"- {li_text}\")\n",
    "\n",
    "    content = '\\n'.join(content_lines)\n",
    "\n",
    "    # Combine into dict\n",
    "    result = {\n",
    "        \"title\": title,\n",
    "        \"date\": post_date,\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hrefs = []\n",
    "\n",
    "for page_num in range(1, 192):\n",
    "    url = f\"https://nik.art/page/{page_num}\"\n",
    "    if page_num % 10 == 0:\n",
    "        print(f'Current page: {page_num}')\n",
    "    html = fetch_page(url)\n",
    "    hrefs = extract_hrefs_from_page(html)\n",
    "\n",
    "\n",
    "    all_hrefs.extend(hrefs)\n",
    "    page_num += 1\n",
    "    \n",
    "unique_hrefs = set(all_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = []\n",
    "\n",
    "for href in list(unique_hrefs): \n",
    "    txt = fetch_page(href)\n",
    "    post_data = extract_blog_post_data(txt)\n",
    "    all_posts.append(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts.sort(key=lambda x: x['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_posts.json\", \"w\") as f:\n",
    "    json.dump(all_posts, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create jsonl training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "raw = load_dataset(\"json\", data_files=\"all_posts.json\", split=\"train\")\n",
    "\n",
    "# Training data: before 2025\n",
    "train_data = raw.filter(lambda ex: ex[\"date\"] < datetime(2025, 1, 1))\n",
    "MAX_LEN = 4000\n",
    "train_data = train_data.filter(lambda ex: len(tokenizer.tokenize(ex[\"content\"])) <= MAX_LEN)\n",
    "\n",
    "# Validation data: 2025 and onward\n",
    "val_data = raw.filter(lambda ex: ex[\"date\"] >= datetime(2025, 1, 1))\n",
    "val_data = val_data.filter(lambda ex: len(tokenizer.tokenize(ex[\"content\"])) <= MAX_LEN)\n",
    "\n",
    "def join_title_body(ex):\n",
    "    return {\"text\": f\"### Title: {ex['title']}\\n\\n### Content: \\n{ex['content']}\" + EOS_TOKEN}\n",
    "\n",
    "# Prepare and save training data\n",
    "train_dataset = train_data.map(join_title_body, remove_columns=train_data.column_names)\n",
    "train_dataset.to_json(\"training_data_before_2025.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# Prepare and save validation data\n",
    "val_dataset = val_data.map(join_title_body, remove_columns=val_data.column_names)\n",
    "val_dataset.to_json(\"val_data_2025_onward.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
